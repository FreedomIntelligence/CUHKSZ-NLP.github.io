---
---

@article{su2022embedder,
  title={One Embedder, Any Task: Instruction-Finetuned Text Embeddings},
  author={Su, Hongjin and Shi, Weijia and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A and Zettlemoyer, Luke and Yu, Tao},
  journal={Preprint},
  year={2022},
  arxiv={2212.09741},
  code={https://github.com/HKUNLP/instructor-embedding},
  data={https://huggingface.co/hkunlp/instructor-large},
  poster={https://instructor-embedding.github.io},
}

@article{lai2022ds,
  title={DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Scott Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  journal={Preprint},
  year={2022},
  arxiv={2211.11501},
  code={https://github.com/HKUNLP/DS-1000},
  poster={https://ds1000-code-gen.github.io/},
  data={https://github.com/HKUNLP/DS-1000/tree/main/ds1000_example},
  selected={y}
}


@inproceedings{liu2022augmenting,
  title={Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play},
  author={Liu, Qi and Ye, Zihuiwen and Yu, Tao and Blunsom, Phil and Song, Linfeng},
  booktitle={Findings of EMNLP 2022},
  year={2022},
  note={Long Paper},
  arxiv={2210.12096},
  code={https://github.com/leuchine/self_play_picard}
}

@inproceedings{cheng2023binding,
  title={Binding Language Models in Symbolic Languages},
  author={Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and Yu, Tao},
  booktitle={International Conference on Learning Representations (ICLR 2023)},
  location={Berlin, Germany},
  year={2023},
  arxiv={2210.02875},
  code={https://lm-code-binder.github.io/},
  poster={https://lm-code-binder.github.io/},
  selected={y}
}

@inproceedings{su2023selective,
  title={Selective Annotation Makes Language Models Better Few-Shot Learners},
  author={Su, Hongjin and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
  booktitle={International Conference on Learning Representations (ICLR 2023)},
  year={2023},
  arxiv={2209.01975},
  code={https://github.com/HKUNLP/icl-selective-annotation}
}

@article{wang2022evaluating,
  title={Evaluating Self-Supervised Learning for Molecular Graph Embeddings},
  author={Wang, Hanchen and Kaddour, Jean and Liu, Shengchao and Tang, Jian and Kusner, Matt and Lasenby, Joan and Liu, Qi},
  journal={arXiv preprint arXiv:2206.08005},
  year={2022},
  arxiv={2206.08005}
}

@article{wang2022augmenting,
  title={Retrieval-enhanced Graph Neural Networks for Graph Property Prediction},
  author={Wang, Dingmin and Liu, Shengchao and Wang, Hanchen and Song, Linfeng and Tang, Jian and Le, Song and Grau, Bernardo Cuenca and Liu, Qi},
  journal={arXiv preprint arXiv:2206.00362},
  year={2022},
  arxiv={2206.00362}
}

@inproceedings{ye2022zerogen,
  title={ZeroGen: Efficient Zero-shot Learning via Dataset Generation},
  author={Ye, Jiacheng and Gao, Jiahui and Li, Qintong and Xu, Hang and Feng, Jiangtao and Wu, Zhiyong and Yu, Tao and Kong, Lingpeng},
  booktitle={Empirical Methods in Natural Language Processing (EMNLP 2022)},
  year={2022},
  arxiv={2202.07922},
  code={https://github.com/jiacheng-ye/ZeroGen}
}

@inproceedings{xie2022unifiedskg,
  title={UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models},
  author={Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida and Zhong, Victor and Wang, Bailin and Li, Chengzu and Boyle, Connor and Ni, Ansong and Yao, Ziyu and Radev, Dragomir and Xiong, Caiming and Kong, Lingpeng and Zhang, Rui and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
  booktitle={Empirical Methods in Natural Language Processing (EMNLP 2022)},
  year={2022},
  arxiv={2201.05966},
  code={https://github.com/hkunlp/unifiedskg},
  poster={https://unifiedskg.com/}
}

@inproceedings{zheng2021cascaded,
  title={Cascaded Head-colliding Attention},
  author={Lin Zheng and Zhiyong Wu and Lingpeng Kong},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2021)},
  year={2021},
  arxiv={2105.14850},
  code={https://github.com/LZhengisme/CODA},
}

@inproceedings{wu2021good,
  title={Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation},
  author={Zhiyong Wu and Lingpeng Kong and Wei Bi and Xiang Li and Ben Kao},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2021)},
  year={2021},
  arxiv={2105.14462},
  code={https://github.com/LividWo/Revisit-MMT},
}

@inproceedings{zheng2022linear,
  title={Linear Complexity Randomized Self-attention Mechanism},
  author={Lin Zheng and Chong Wang and Lingpeng Kong},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML 2022)},
  year={2022},
  arxiv={2204.04667},
  code={https://github.com/HKUNLP/efficient-attention},
}

@inproceedings{zheng2022ripple,
  title={Ripple Attention for Visual Perception with Sub-quadratic Complexity},
  author={Lin Zheng and Huijie Pan and Lingpeng Kong},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML 2022)},
  year={2022},
  arxiv={2110.02453}
}

@inproceedings{prange2022linguistic,
  title={Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling},
  author={Jakob Prange and Nathan Schneider and Lingpeng Kong},
  booktitle={Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2022)},
  year={2022},
  arxiv={2112.07874},
  code={https://github.com/jakpra/LinguisticStructureLM},
}

@inproceedings{li2022event,
  title={Event Transition Planning for Open-ended Text Generation},
  author={Qintong Li and Piji Li and Wei Bi and Zhaochun Ren and Yuxuan Lai and Lingpeng Kong},
  booktitle={Findings of the Annual Meeting of the Association for Computational Linguistics (ACL 2022 Findings)},
  year={2022},
  arxiv={2204.09453},
  code={https://github.com/qtli/EventPlanforTextGen},
}

@inproceedings{wu2022lexical,
  title={Lexical Knowledge Internalization for Neural Dialog Generation},
  author={Zhiyong Wu and Wei Bi and Xiang Li and Lingpeng Kong and Ben Kao},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2022)},
  year={2022},
  arxiv={2205.01941},
  code={https://github.com/LividWo/KI},
}

@inproceedings{ye2022progen,
  title={ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback},
  author={Jiacheng Ye and Jiahui Gao and Zhiyong Wu and Jiangtao Feng and Tao Yu and Lingpeng Kong},
  booktitle={Findings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2022 Findings)},
  location={Abu Dhabi},
  year={2022},
  arxiv={2210.12329},
  code={https://github.com/HKUNLP/ProGen},
}

@inproceedings{zheng2023efficient,
  title={Efficient Attention via Control Variates},
  author={Lin Zheng and Jianbo Yuan and Chong Wang and Lingpeng Kong},
  booktitle={International Conference on Learning Representations (ICLR 2023)},
  location={Kigali, Rwanda},
  year={2023},
  arxiv={2302.04542},
  code={https://github.com/HKUNLP/efficient-attention},
}

@inproceedings{gao2023selfguided,
  title={Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning},
  author={Jiahui Gao and Renjie Pi and Lin Yong and Hang Xu and Jiacheng Ye and Zhiyong Wu and Weizhong Zhang and Xiaodan Liang and Zhenguo Li and Lingpeng Kong},
  booktitle={International Conference on Learning Representations (ICLR 2023)},
  location={Kigali, Rwanda},
  year={2023},
  arxiv={2205.12679},
  code={https://github.com/SumilerGAO/SunGen},
}

@inproceedings{gong2023diffuseq,
  title={DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models},
  author={Shansan Gong and Mukai Li and Jiangtao Feng and Zhiyong Wu and Lingpeng Kong},
  booktitle={International Conference on Learning Representations (ICLR 2023)},
  location={Kigali, Rwanda},
  year={2023},
  arxiv={2210.08933},
  code={https://github.com/Shark-NLP/DiffuSeq},
}

@inproceedings{chen2023unsupervised,
  title={Unsupervised Explanation Generation via Correct Instantiations},
  author={Sijie Chen and Zhiyong Wu and Jiangjie Chen and Zhixing Li and Yang Liu and Lingpeng Kong},
  booktitle={Proceedings of AAAI Conference on Artificial Intelligence (AAAI 2023)},
  location={Washington, DC},
  year={2023},
  arxiv={2211.11160},
  code={https://github.com/Shark-NLP/Neon},
}

@article{ma2023retrieved,
  title={Retrieved Sequence Augmentation for Protein Representation Learning},
  author={Chang Ma and Haiteng Zhao and Lin Zheng and Jiayi Xin and Qintong Li and Lijun Wu and Zhihong Deng and Yang Lu and Qi Liu and Lingpeng Kong},
  journal={arXiv preprint arXiv:2302.12563},
  year={2023},
  arxiv={2302.12563},
  code={https://github.com/HKUNLP/RSA},
}

@article{zheng2023reparameterized,
  title={A Reparameterized Discrete Diffusion Model for Text Generation},
  author={Lin Zheng and Jianbo Yuan and Lei Yu and Lingpeng Kong},
  journal={arXiv preprint arXiv:2302.04542},
  year={2023},
  arxiv={2302.04542},
  code={https://github.com/HKUNLP/reparam-discrete-diffusion},
}

@article{ye2023compositional,
  title={Compositional Exemplars for In-context Learning},
  author={Jiacheng Ye and Zhiyong Wu and Jiangtao Feng and Tao Yu and Lingpeng Kong},
  journal={arXiv preprint arXiv:2302.05698},
  year={2023},
  arxiv={2302.05698},
  code={https://github.com/HKUNLP/icl-ceil},
}

@article{wu2022selfadaptive,
  title={Self-adaptive In-context Learning},
  author={Zhiyong Wu and Yaoxiang Wang and Jiacheng Ye and Lingpeng Kong},
  journal={arXiv preprint arXiv:2212.10375},
  year={2022},
  arxiv={2212.10375},
  code={https://github.com/Shark-NLP/self-adaptive-ICL},
}

@article{zhang2022cab,
  title={CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling},
  author={Jun Zhang and Shuyang Jiang and Jiangtao Feng and Lin Zheng and Lingpeng Kong},
  journal={arXiv preprint arXiv:2210.07661},
  year={2022},
  arxiv={2210.07661},
  code={https://github.com/Shark-NLP/CAB},
}